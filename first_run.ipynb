{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdf826a-6080-4dc5-8f5c-1dcd726af5dd",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align:center;\">Welcome to the <b><span style=\"color:green\">Upply Notebook</span></b>, a notebook dedicated to using OpenAI's ChatGPT (4o-mini) to help generate text for proposals on the freelance site 'Upwork'.</p> \n",
    "##### <p style=\"text-align:center;\">The purpose of this notebook is to <i><span style=\"color:red\">generate seemless prompts</span></i> that are good enough to feel organic and natural, and in no-way AI generated.</p>\n",
    "#### <p style=\"text-align:center;\"><u>A breakdown of what this entails:</u></p>\n",
    "#### 1.) Bringing in a Markdown version of my resume for ChatGPT to reference;\n",
    "#### 2.) Taking in the Job Description from the Upwork Website Via Copy/Paste;\n",
    "#### 3.) Constructing A Prompt Template with ChatGPT; and\n",
    "#### 4.) Generating A Natural-Sounding Proposal\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4d7f9a-f595-41a8-bdd9-88e39ecfde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and foremost, though: import the python libraries we'll need\n",
    "\n",
    "# to access environmental openai key variable housed in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# check - should come back 'True'\n",
    "# load_dotenv()\n",
    "\n",
    "# for rich displays in Markdown becuase it is dynamic, unboring text\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# use Python's OpenAI package to access the OpenAI API\n",
    "from openai import OpenAI\n",
    "\n",
    "# we want to convert the markdown to HTML for the site's use\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf09d1-a8d2-407f-9a76-59e5192a3e56",
   "metadata": {},
   "source": [
    "***\n",
    "#### Step 1: Bringing in a Markdown version of my resume for ChatGPT to reference\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d372a5-4f0f-49ae-948c-709895efcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up the saved Markdown resume and read it\n",
    "with open(\"/Users/nicholasjoseph/Desktop/nj_pm_res.md\", \"r\", encoding = \"utf-8\") as resume_file:\n",
    "    resume_string = resume_file.read()\n",
    "\n",
    "# check\n",
    "# display(Markdown(resume_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14d158-0b5a-4201-9d1a-8100b5fa0643",
   "metadata": {},
   "source": [
    "##### the check looked good. python's reading the resume as expected. Now onto:\n",
    "***\n",
    "#### Step 2: Taking In the Job Description from the Upwork Website Via Copy / Paste\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d8f8a4-98a1-4998-b97d-63ff086ae188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Project Overview:  We are building an ambitious and innovative Modular Knowledge Graph intelligence Platform. Our vision is to improve on data analysis and software development tools by creating a flexible, adaptable, and self-improving system.  This platform aims to integrate different knowledge sources, provide deeper causal insights (from descriptive \"what happened\" to intervention \"what to do\"), and dynamically evolve the underlying knowledge graph as new insights are discovered. It leverages formal ontologies and modular \"operator\" principles for sophisticated analysis.  This is a unique project. There is literally nothing like it. If you can find anything similar, send me a link and I’ll send you $100. While we are able to code this ourselves, progress has been 2 steps forward 1 step back, and we are looking to speed it up.  With that being said, this is not a traditional role. It’s hard to know what skill sets we are looking for, the workload, and honestly, I’m a bit skeptical of finding someone that’s a good fit for this due to it being unconventional. But ideally, you will have some familiarity or at least understand the potential and purpose of such a system.  We are open to just hiring someone to consult on this as well. I apologize as normally I like to give more detailed specs on what you’d be doing, but while the concept is thoroughly mapped out, we are figuring out how to build it as we go.  There is a lot of potential work for the right person, but we’ll need to sort those details after meeting.  Pay: $50-75/hr - We are flexible on this if you prefer to do a project rate.  Links:  Here is a demo of where we are at on a simple calculator in regards to the autocoder: https://www.youtube.com/watch?v=kzPfGwpjxSs  Here is an explanation of the bigger vision: https://www.youtube.com/watch?v=p_YJMa08d94  Autocoder System Details:  A critical part of this platform is the Autocoder System – the component you would be primarily working on initially. The Autocoder is a methodology and toolset for developing software systems. It uses a modular, visual, diagram-driven approach where:  Systems are broken down into smaller components with defined interfaces (input/output ports with specific data schemas). These components are represented in diagrams that specify the system's structure and data flow. The Autocoder utilizes LLMs to help generate these diagrams and the code for each component, allowing them to be tested in isolation before integration based on the diagram. This modular approach helps manage complexity and work effectively with LLMs in the context of larger codebases.  Current Status:  We are currently building and stabilizing the core Autocoder system (~10,000+ lines of Python code). Its reliability is important for the larger vision.  The Challenge & Nature of the Work:  Addressing detailed implementation issues common in managing an interconnected codebase, especially one involving LLM generation. Including:  Debugging Interconnected Issues: Fixing a bug or inconsistency in one area can sometimes cause problems elsewhere. So tracing these effects and ensuring fixes are comprehensive.  Ensuring Consistency: Maintaining consistency in variable/class/file naming conventions, string formatting, and import paths across the Autocoder system.  Working with LLM-Generated Code: The role involves verifying code generated by LLMs for the Autocoder components, identifying errors or inconsistencies, and implementing manual corrections or providing specific context for regeneration attempts. Familiarity with prompt engineering is helpful, but we need code-level verification/correction over solely focusing on prompt engineering.  What we are looking for: Python Experience: Understanding of Python and experience writing/debugging applications.  Debugging Skills: Ability to diagnose problems, trace issues across files/modules, and identify root causes in complex code.  Willingness to Learn the Codebase: Commitment to spending the necessary time to understand the existing Autocoder code and its architecture (e.g., Component-Based Data Flow).  Next Steps: Please just send me a brief explanation on why you'd be a good fit, and some examples of your work.\n"
     ]
    }
   ],
   "source": [
    "# simple enough because it'll be a physical copy / paste\n",
    "job_description_string = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beaaddc-9e92-45f7-b9bc-8132a26d662e",
   "metadata": {},
   "source": [
    "***\n",
    "#### Step 3: Constructing A Prompt Template with ChatGPT\n",
    "##### 'lambda' function is being used here because making the return statement all the prompt text seems ridiculous.\n",
    "##### also, this is a template I prompted ChatGPT with - reason being, let Chat generate it because LLMs talk to LLMs. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab86969-59c1-422e-bfbf-163798d4f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially, this is a large formatted string assigned to a single variable\n",
    "prompt_template = lambda resume_string, job_description_string : f\"\"\"\n",
    "You are a professional proposal writer writing for clients looking for freelance work on Upwork.\n",
    "\n",
    "Your job is to read a freelancer's resume and a specific job description, then craft a personalized proposal for that job. The proposal should highlight the freelancer's most relevant experience, skills, and accomplishments as they relate to the job description.\n",
    "\n",
    "Use the following guidelines:\n",
    "\n",
    "1. **Tone**: Blend personable warmth with technical competence. Show that the freelancer is both approachable and highly capable.\n",
    "2. **Content focus**: Prioritize skills, tools, and accomplishments that most closely match the job description.\n",
    "3. **Language**: Use confident, active, and natural language with strong action verbs and quantifiable results wherever possible (e.g., \"boosted revenue by 40%\", \"reduced processing time by 60%\", etc.).\n",
    "4. **Length**: Keep the proposal concise (under 200 words), but meaningful.\n",
    "5. **Close strong**: End with a short paragraph inviting the client to connect, ask questions, or discuss the project further.\n",
    "6. **Integrate **keywords** and phrases from the job description naturally to give the text an 'I-read-over-your-ad' type of feel.\n",
    "7. **Output the tailored resume in the appropriate Upwork proposal format that is clean and free from errors.\n",
    "\n",
    "---\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "RESUME:\n",
    "<<<\n",
    "{resume_string}\n",
    ">>>\n",
    "\n",
    "JOB DESCRIPTION:\n",
    "<<<\n",
    "{job_description_string}\n",
    ">>>\n",
    "\n",
    "---\n",
    "\n",
    "TASK:\n",
    "Write a complete Upwork proposal tailored to the job description above. Emphasize alignment between the freelancer's background and the project requirements. Don't include unnecessary fluff—make every sentence purposeful and impactful.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9d2ead-a282-49f8-8484-bc7fd6540941",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template(resume_string, job_description_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2b18d-0989-4d2c-9939-ce18fdb6bf8f",
   "metadata": {},
   "source": [
    "***\n",
    "#### Step 4: Generating A Natural-Sounding Proposal\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0642c35-b409-4069-b2ad-534277107556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BOlze33Zx81PJW9hfRvh3XLCWO0FL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Aloha!\\n\\nI am excited about the opportunity to contribute to your innovative Modular Knowledge Graph intelligence platform. With over 7 years as a Product Manager and Cloud Data Engineer, I have a strong foundation in Python programming and extensive experience in managing and debugging interconnected codebases, particularly during my time at Spaulding Ridge. There, I successfully led the migration of complex data systems, which improved efficiency by 98.9%, and collaborated closely with cross-functional teams to ensure project alignment and success.\\n\\nMy technical background in data engineering, combined with my ability to translate intricate data workflows into user-friendly solutions, makes me well-suited to tackle the challenges of your Autocoder system. I am adept at diagnosing issues across multiple layers of code and maintaining consistency in naming conventions and data schemas, ensuring a reliable and robust architecture.\\n\\nI am eager to dive into your existing codebase, understand its architecture, and help accelerate the development of this groundbreaking project. Let’s connect to discuss how I can contribute to your vision!\\n\\nLooking forward to hearing from you.\\n\\nBest,  \\nNicholas Joseph  \\nnickpjoseph210@gmail.com  \\n(210) 771-9853  ', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1745244058, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_f7d56a8a2c', usage=CompletionUsage(completion_tokens=232, prompt_tokens=2101, total_tokens=2333, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the api clent\n",
    "\n",
    "open_apikey = os.environ.get(\"openai_apikey\")\n",
    "\n",
    "client = OpenAI(api_key = open_apikey)\n",
    "\n",
    "# call the api\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"Expert Proposal Writer\"},\n",
    "        {\"role\" : \"user\", \"content\" : prompt}\n",
    "    ], \n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "response\n",
    "# see the response\n",
    "# response_string = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b52ca26a-18f0-4ce6-ac3f-a78caae3216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd9ec2-be01-4426-b5af-8c524b79e632",
   "metadata": {},
   "source": [
    "***\n",
    "#### Since the response is \"openai.types.chat.chat_completion.ChatCompletion\" and we want to put this into a markdown file\n",
    "#### for editing, we want to extract the relevant data, iterate through choices, format said choices, and finally create\n",
    "#### the Markdown file for export.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901f824b-6bd9-4a55-a4e5-28c89ba86a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat Completion response exported to: /Users/nicholasjoseph/PromptEngineering/api_work/upwork_apply/upwork_apply/openai_response.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "response_data = {\n",
    "    \"id\": response.id,\n",
    "    \"model\": response.model,\n",
    "    \"created\": response.created,\n",
    "    \"object\": response.object,\n",
    "    \"choices\": []\n",
    "}\n",
    "\n",
    "for choice in response.choices:\n",
    "    choice_data = {\n",
    "        \"index\": choice.index,\n",
    "        \"finish_reason\": choice.finish_reason,\n",
    "        \"role\": choice.message.role,\n",
    "        \"content\": choice.message.content\n",
    "    }\n",
    "    response_data[\"choices\"].append(choice_data)\n",
    "\n",
    "markdown_content = \"# OpenAI Chat Completion Response\\n\\n\"\n",
    "\n",
    "markdown_content += f\"- **ID:** {response_data['id']}\\n\"\n",
    "markdown_content += f\"- **Model:** {response_data['model']}\\n\"\n",
    "markdown_content += f\"- **Created (Timestamp):** {response_data['created']}\\n\"\n",
    "markdown_content += f\"- **Object:** {response_data['object']}\\n\\n\"\n",
    "\n",
    "markdown_content += \"## Choices:\\n\\n\"\n",
    "for choice in response_data['choices']:\n",
    "    markdown_content += f\"### Choice {choice['index'] + 1}\\n\\n\"\n",
    "    markdown_content += f\"- **Finish Reason:** {choice['finish_reason']}\\n\"\n",
    "    markdown_content += f\"- **Role:** {choice['role']}\\n\"\n",
    "    markdown_content += f\"- **Content:**\\n\"\n",
    "    markdown_content += \"```\\n\"\n",
    "    markdown_content += choice['content'] + \"\\n\"\n",
    "    markdown_content += \"```\\n\\n\"\n",
    "\n",
    "output_filename = \"openai_response.md\"\n",
    "output_path = os.path.join(os.getcwd(), output_filename)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print(f\"OpenAI Chat Completion response exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d216d2-3fa4-4326-806d-1be68e3cbc90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
